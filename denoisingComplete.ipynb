{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "717ddc2d43fc401796a7e81e6e804605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_601a5db08b1d46e19f83a60561439d0e",
              "IPY_MODEL_d7e80ae2d21c4fee956a245013c5fb9f",
              "IPY_MODEL_68db2d760a364cc4b101bf6e1ad03ccc"
            ],
            "layout": "IPY_MODEL_c9b9bc5b72cd44f78bb3cfd0dbcfa6cc"
          }
        },
        "601a5db08b1d46e19f83a60561439d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d4b2f4262ab4e18859af0774102da71",
            "placeholder": "​",
            "style": "IPY_MODEL_886354b025f5473ca4f135fbafc5feb6",
            "value": "Epoch 1/20:   0%"
          }
        },
        "d7e80ae2d21c4fee956a245013c5fb9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b50857cef60d4926b638eff1d3fec118",
            "max": 132,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_205d855906da4323bf140954e6483e55",
            "value": 0
          }
        },
        "68db2d760a364cc4b101bf6e1ad03ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_058b8dd5a4794ae4a6e615eb0cc98de0",
            "placeholder": "​",
            "style": "IPY_MODEL_dc531dbfeb8448f58d35a1233a97b903",
            "value": " 0/132 [00:00&lt;?, ?it/s]"
          }
        },
        "c9b9bc5b72cd44f78bb3cfd0dbcfa6cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d4b2f4262ab4e18859af0774102da71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886354b025f5473ca4f135fbafc5feb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b50857cef60d4926b638eff1d3fec118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "205d855906da4323bf140954e6483e55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "058b8dd5a4794ae4a6e615eb0cc98de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc531dbfeb8448f58d35a1233a97b903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. MOUNT GOOGLE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxVPhHZNSXOl",
        "outputId": "1db48026-3dd1-49a6-9759-ffb189050e35"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq7XqZViO7gK",
        "outputId": "12d94ebb-4c21-4ee2-c41e-fdf17cb971e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NSIN\n",
            "Running on device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 1337\n",
        "BASE_DRIVE_PATH = '/content/drive/MyDrive/NSIN' # Change if needed\n",
        "print(BASE_DRIVE_PATH)\n",
        "\n",
        "def set_seed(seed: int = 1337):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"Running on device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PairedNISN(Dataset):\n",
        "    \"\"\"\n",
        "    Loads paired noisy and clean images from a directory structure.\n",
        "    Expected structure:\n",
        "      root/\n",
        "        noisy images/*.jpg\n",
        "        ground truth/*.jpg\n",
        "    \"\"\"\n",
        "    def __init__(self, split_root: str, resize_to=None):\n",
        "        self.root = Path(split_root)\n",
        "        self.noisy_dir = self.root / \"noisy images\"\n",
        "        self.clean_dir = self.root / \"ground truth\"\n",
        "\n",
        "        # Basic validation\n",
        "        if not self.noisy_dir.exists() or not self.clean_dir.exists():\n",
        "            print(f\"Warning: Directories not found in {split_root}\")\n",
        "\n",
        "        # Transforms\n",
        "        t = []\n",
        "        if resize_to is not None:\n",
        "            t.append(transforms.Resize(resize_to))\n",
        "        t.append(transforms.ToTensor())\n",
        "        self.transform = transforms.Compose(t)\n",
        "\n",
        "        # Find files\n",
        "        exts = (\"*.jpg\", \"*.jpeg\", \"*.png\")\n",
        "        self.pairs = []\n",
        "\n",
        "        # Index clean images by filename for fast lookup\n",
        "        clean_index = {}\n",
        "        if self.clean_dir.exists():\n",
        "            for e in exts:\n",
        "                for p in self.clean_dir.glob(e):\n",
        "                    clean_index[p.name] = p\n",
        "\n",
        "        # Match noisy images to clean images\n",
        "        if self.noisy_dir.exists():\n",
        "            noisy_paths = []\n",
        "            for e in exts:\n",
        "                noisy_paths.extend(self.noisy_dir.glob(e))\n",
        "\n",
        "            for n_path in sorted(noisy_paths):\n",
        "                # Logic: gauss_123_abc.jpg -> 123_abc.jpg\n",
        "                # Adjust this logic if your naming convention differs\n",
        "                clean_name = n_path.name\n",
        "                if '_' in clean_name:\n",
        "                     clean_name = clean_name[clean_name.find('_')+1:]\n",
        "\n",
        "                c_path = clean_index.get(clean_name)\n",
        "                if c_path:\n",
        "                    self.pairs.append((n_path, c_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        n_path, c_path = self.pairs[idx]\n",
        "        noisy = Image.open(n_path).convert(\"RGB\")\n",
        "        clean = Image.open(c_path).convert(\"RGB\")\n",
        "        return self.transform(noisy), self.transform(clean)"
      ],
      "metadata": {
        "id": "UeIOY5_yPTVM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(Conv -> BN -> ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"MaxPool -> DoubleConv\"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        return self.conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscale -> Concat -> DoubleConv\"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        # Handle padding issues if dimensions don't match perfectly\n",
        "        diffY = skip.size(2) - x.size(2)\n",
        "        diffX = skip.size(3) - x.size(3)\n",
        "        if diffY > 0 or diffX > 0:\n",
        "            x = F.pad(x, [diffX // 2, diffX - diffX // 2,\n",
        "                          diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([skip, x], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=3, base=64):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(in_ch, base)\n",
        "        self.down1 = Down(base, base*2)\n",
        "        self.down2 = Down(base*2, base*4)\n",
        "        self.down3 = Down(base*4, base*8)\n",
        "        self.down4 = Down(base*8, base*16)\n",
        "        self.up1 = Up(base*16, base*8)\n",
        "        self.up2 = Up(base*8, base*4)\n",
        "        self.up3 = Up(base*4, base*2)\n",
        "        self.up4 = Up(base*2, base)\n",
        "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
        "        self.act = nn.Sigmoid() # Output [0, 1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return self.act(logits)"
      ],
      "metadata": {
        "id": "cDc2ZZ9NPWLG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, l1_weight=1.0, vgg_weight=0.1, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.l1_weight = l1_weight\n",
        "        self.vgg_weight = vgg_weight\n",
        "\n",
        "        # Standard L1 Loss\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "\n",
        "        # VGG Perceptual Loss Setup\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        # We extract features from specific layers to capture texture/perceptual info\n",
        "        # indices 2, 7, 12, 21, 30 usually correspond to relu1_2, relu2_2, etc.\n",
        "        self.feature_layers = [2, 7, 12, 21, 30]\n",
        "        self.feature_extractor = nn.ModuleList()\n",
        "\n",
        "        # Extract layers and freeze them\n",
        "        for i, layer in enumerate(vgg):\n",
        "            if i in self.feature_layers:\n",
        "                # Wrap previous layers + current layer into a Sequential for easier extraction\n",
        "                # Note: This implementation just grabs specific layers.\n",
        "                # A simpler way is to pass input through full VGG and hook outputs,\n",
        "                # but extracting layers is efficient for inference.\n",
        "                pass\n",
        "\n",
        "        # A more robust VGG extractor for this context:\n",
        "        self.vgg_submodules = vgg[:max(self.feature_layers) + 1].to(device).eval()\n",
        "        for param in self.vgg_submodules.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # ImageNet normalization stats\n",
        "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device))\n",
        "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device))\n",
        "\n",
        "    def get_vgg_features(self, x):\n",
        "        \"\"\"Forward pass through VGG layers, collecting features.\"\"\"\n",
        "        features = []\n",
        "        # Normalize first\n",
        "        x = (x - self.mean) / self.std\n",
        "        for i, layer in enumerate(self.vgg_submodules):\n",
        "            x = layer(x)\n",
        "            if i in self.feature_layers:\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # 1. L1 Pixel Loss\n",
        "        loss_l1 = self.l1_loss(pred, target)\n",
        "\n",
        "        # 2. VGG Perceptual Loss\n",
        "        pred_feats = self.get_vgg_features(pred)\n",
        "        target_feats = self.get_vgg_features(target)\n",
        "\n",
        "        loss_vgg = 0.0\n",
        "        for pf, tf in zip(pred_feats, target_feats):\n",
        "            loss_vgg += F.l1_loss(pf, tf)\n",
        "\n",
        "        # Combine\n",
        "        total_loss = (self.l1_weight * loss_l1) + (self.vgg_weight * loss_vgg)\n",
        "        return total_loss, loss_l1.item(), loss_vgg.item()"
      ],
      "metadata": {
        "id": "UgkBswcHPauM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=20, lr=1e-4):\n",
        "    # Initialize Loss\n",
        "    # vgg_weight=0.1 is a common starting point; adjust if artifacts appear\n",
        "    criterion = CombinedLoss(l1_weight=1.0, vgg_weight=0.1, device=DEVICE)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    # Gradient Scaler for Mixed Precision (faster on T4/A100)\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "    save_dir = Path(\"./checkpoints\")\n",
        "    save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "        for noisy, clean in pbar:\n",
        "            noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with Mixed Precision\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                pred = model(noisy)\n",
        "                loss, l1_val, vgg_val = criterion(pred, clean)\n",
        "\n",
        "            # Backward pass\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            pbar.set_postfix({\"L1\": f\"{l1_val:.4f}\", \"VGG\": f\"{vgg_val:.4f}\", \"Total\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch} Train Loss: {avg_loss:.5f}\")\n",
        "\n",
        "        # Validation\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for v_noisy, v_clean in val_loader:\n",
        "                    v_noisy, v_clean = v_noisy.to(DEVICE), v_clean.to(DEVICE)\n",
        "                    v_pred = model(v_noisy)\n",
        "                    v_loss, _, _ = criterion(v_pred, v_clean)\n",
        "                    val_loss += v_loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Epoch {epoch} Val Loss:   {avg_val_loss:.5f}\")\n",
        "\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), save_dir / \"last.pth\")\n",
        "            scheduler.step(avg_val_loss)\n"
      ],
      "metadata": {
        "id": "Paol8_5-PdFe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Execution ---\n",
        "\n",
        "# 1. Create Datasets\n",
        "# Ensure these paths exist in your Google Drive\n",
        "train_ds = PairedNISN(f\"{BASE_DRIVE_PATH}/train/train\")\n",
        "val_ds   = PairedNISN(f\"{BASE_DRIVE_PATH}/validate/validate\")\n",
        "\n",
        "if len(train_ds) == 0:\n",
        "    print(\"No training data found. Check BASE_DRIVE_PATH.\")\n",
        "else:\n",
        "    # 2. Create Loaders\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # 3. Initialize Model\n",
        "    model = UNet().to(DEVICE)"
      ],
      "metadata": {
        "id": "MVThWK7_PeIC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # 4. Run Training\n",
        "    # The model will train using L1 + VGG loss.\n",
        "    train_model(model, train_loader, val_loader, epochs=20, lr=2e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "717ddc2d43fc401796a7e81e6e804605",
            "601a5db08b1d46e19f83a60561439d0e",
            "d7e80ae2d21c4fee956a245013c5fb9f",
            "68db2d760a364cc4b101bf6e1ad03ccc",
            "c9b9bc5b72cd44f78bb3cfd0dbcfa6cc",
            "3d4b2f4262ab4e18859af0774102da71",
            "886354b025f5473ca4f135fbafc5feb6",
            "b50857cef60d4926b638eff1d3fec118",
            "205d855906da4323bf140954e6483e55",
            "058b8dd5a4794ae4a6e615eb0cc98de0",
            "dc531dbfeb8448f58d35a1233a97b903"
          ]
        },
        "id": "pFaOvRS9RX5R",
        "outputId": "0b7fa333-4f8e-4641-d80d-893fc81490e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548M/548M [00:02<00:00, 244MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/20:   0%|          | 0/132 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "717ddc2d43fc401796a7e81e6e804605"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}