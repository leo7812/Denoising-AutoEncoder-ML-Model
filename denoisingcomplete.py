# -*- coding: utf-8 -*-
"""denoisingComplete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZM3VoJ4qJGH2MuvzRvYB7jnqhDxvAlMf
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import random
import math
from pathlib import Path
from datetime import datetime
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.autograd import Variable
from torchvision import transforms, models
from torch.optim.lr_scheduler import ReduceLROnPlateau
from PIL import Image
from tqdm.auto import tqdm
import shutil
import torchvision.transforms.functional as TF

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SEED = 1337

DRIVE_ARCHIVE_PATH = '/content/drive/MyDrive/NSIN.zip'
DRIVE_FOLDER_PATH  = '/content/drive/MyDrive/NSIN'

LOCAL_DATA_PATH = '/content/NSIN'

print(f"Running on device: {DEVICE}")

def setup_data():
    if os.path.exists(LOCAL_DATA_PATH):
        print(f"Data already exists at {LOCAL_DATA_PATH}")
        return LOCAL_DATA_PATH

    print("Setting up data locally for faster training...")

    if os.path.exists(DRIVE_ARCHIVE_PATH):
        print(f"   Copying zip from {DRIVE_ARCHIVE_PATH}...")
        shutil.copy(DRIVE_ARCHIVE_PATH, '/content/NSIN.zip')

        print("   Unzipping...")
        shutil.unpack_archive('/content/NSIN.zip', '/content')

        # Cleanup to save space
        os.remove('/content/NSIN.zip')
        print("Done!")

    # if you only have the folder (Slower copy, but still faster training)
    elif os.path.exists(DRIVE_FOLDER_PATH):
        print(f" No zip found. Copying folder from {DRIVE_FOLDER_PATH} (This might take a few mins)...")
        shutil.copytree(DRIVE_FOLDER_PATH, LOCAL_DATA_PATH)
        print("Done!")

    else:
        raise FileNotFoundError(f"Could not find data at {DRIVE_ARCHIVE_PATH} or {DRIVE_FOLDER_PATH}")

    return LOCAL_DATA_PATH

start_time = datetime.now()
BASE_PATH = setup_data() # usage: train_ds = PairedNISN(f"{BASE_PATH}/train/train")
print(f"Setup took: {datetime.now() - start_time}")

class PairedNISN(Dataset):
    def __init__(self, split_root: str, train_mode=True):
        self.root = Path(split_root)
        self.noisy_dir = self.root / "noisy images"
        self.clean_dir = self.root / "ground truth"
        self.train_mode = train_mode

        # basic validation
        if not self.noisy_dir.exists() or not self.clean_dir.exists():
            print(f"Warning: Directories not found in {split_root}")

        # we ONLY convert to tensor here. All other transforms happen in __getitem__
        self.to_tensor = transforms.ToTensor()

        exts = ("*.jpg", "*.jpeg", "*.png")
        self.pairs = []
        clean_index = {}

        if self.clean_dir.exists():
            for e in exts:
                for p in self.clean_dir.glob(e):
                    clean_index[p.name] = p

        if self.noisy_dir.exists():
            noisy_paths = []
            for e in exts:
                noisy_paths.extend(self.noisy_dir.glob(e))

            for n_path in sorted(noisy_paths):
                clean_name = n_path.name
                if '_' in clean_name:
                     clean_name = clean_name[clean_name.find('_')+1:]
                c_path = clean_index.get(clean_name)
                if c_path:
                    self.pairs.append((n_path, c_path))

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        n_path, c_path = self.pairs[idx]
        noisy = Image.open(n_path).convert("RGB")
        clean = Image.open(c_path).convert("RGB")

        # SYNCHRONIZED TRANSFORMS
        if self.train_mode:
            # 1. Random Crop (Train on 128x128 patches for speed & stability)
            #  we can try 256x256 if your GPU allows, but 128 is standard for denoising.
            i, j, h, w = transforms.RandomCrop.get_params(noisy, output_size=(128, 128))
            noisy = TF.crop(noisy, i, j, h, w)
            clean = TF.crop(clean, i, j, h, w)

            # 2. Random Horizontal Flip
            if random.random() > 0.5:
                noisy = TF.hflip(noisy)
                clean = TF.hflip(clean)

            # 3. Random Vertical Flip
            if random.random() > 0.5:
                noisy = TF.vflip(noisy)
                clean = TF.vflip(clean)

        # for validation, we return the FULL 512x512 image

        return self.to_tensor(noisy), self.to_tensor(clean)

class ResBlock(nn.Module):
    """(Conv -> BN -> ReLU -> Conv -> BN) + Skip Connection"""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        # if input channels != output channels, we need a 1x1 conv to match dimensions
        self.downsample = None
        if in_ch != out_ch:
            self.downsample = nn.Conv2d(in_ch, out_ch, kernel_size=1)

        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x

        out = self.net(x)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity  # "Residual" connection
        return self.relu(out)

class Down(nn.Module):
    """MaxPool -> DoubleConv"""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.conv = ResBlock(in_ch, out_ch)

    def forward(self, x):
        x = self.pool(x)
        return self.conv(x)

class Up(nn.Module):
    """Upscale (Resize) -> Concat -> DoubleConv"""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        # 1. Replace ConvTranspose2d with Upsample + Conv
        # This prevents "checkerboard artifacts" which ruin SSIM scores
        self.up = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            nn.Conv2d(in_ch, in_ch // 2, kernel_size=1) # 1x1 Conv to reduce channels
        )
        self.conv = ResBlock(in_ch, out_ch)

    def forward(self, x, skip):
        x = self.up(x)

        # Padding handling (same as before)
        diffY = skip.size(2) - x.size(2)
        diffX = skip.size(3) - x.size(3)
        if diffY > 0 or diffX > 0:
            x = F.pad(x, [diffX // 2, diffX - diffX // 2,
                          diffY // 2, diffY - diffY // 2])

        x = torch.cat([skip, x], dim=1)
        return self.conv(x)

class UNet(nn.Module):
    def __init__(self, in_ch=3, out_ch=3, base=64):
        super().__init__()
        self.inc = ResBlock(in_ch, base)
        self.down1 = Down(base, base*2)
        self.down2 = Down(base*2, base*4)
        self.down3 = Down(base*4, base*8)
        self.down4 = Down(base*8, base*16)
        self.up1 = Up(base*16, base*8)
        self.up2 = Up(base*8, base*4)
        self.up3 = Up(base*4, base*2)
        self.up4 = Up(base*2, base)
        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)
        self.act = nn.Sigmoid() # Output [0, 1]

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return self.act(logits)

# SSIM Utilities
def gaussian(window_size, sigma):
    gauss = torch.Tensor([math.exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])
    return gauss / gauss.sum()

def create_window(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())
    return window

def _ssim(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)
    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size//2, groups=channel) - mu1_sq
    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size//2, groups=channel) - mu2_sq
    sigma12   = F.conv2d(img1 * img2, window, padding=window_size//2, groups=channel) - mu1_mu2

    C1 = 0.01 ** 2
    C2 = 0.03 ** 2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map.mean(1).mean(1).mean(1)

class SSIM(nn.Module):
    def __init__(self, window_size=11, size_average=True):
        super(SSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window(window_size, self.channel)

    def forward(self, img1, img2):
        (_, channel, _, _) = img1.size()
        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window(self.window_size, channel)
            if img1.is_cuda:
                window = window.cuda(img1.get_device())
            window = window.type_as(img1)
            self.window = window
            self.channel = channel
        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)

class VGGPerceptualLoss(nn.Module):
    def __init__(self, l1_weight=1.0, vgg_weight=0.1, device='cuda'):
        super().__init__()
        self.l1_weight = l1_weight
        self.vgg_weight = vgg_weight
        self.l1 = nn.L1Loss()

        # Load VGG19, pretrained on ImageNet
        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features

        # We only need the first few layers to detect "edges" and "textures"
        # Stopping at layer 35 captures high-level texture without being too abstract
        self.vgg_features = nn.Sequential(*list(vgg.children())[:35]).eval().to(device)

        # Freeze VGG weights (we don't train VGG, we just use it as a judge)
        for param in self.vgg_features.parameters():
            param.requires_grad = False

        # Normalization for VGG (ImageNet stats)
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device))

    def forward(self, pred, target):
        # 1. Pixel-wise accuracy (keeps colors correct)
        loss_l1 = self.l1(pred, target)

        # 2. Perceptual accuracy (keeps textures sharp)
        # Normalize inputs for VGG
        pred_norm = (pred - self.mean) / self.std
        target_norm = (target - self.mean) / self.std

        pred_feat = self.vgg_features(pred_norm)
        target_feat = self.vgg_features(target_norm)

        loss_vgg = F.mse_loss(pred_feat, target_feat)

        return (self.l1_weight * loss_l1) + (self.vgg_weight * loss_vgg), loss_l1.item(), loss_vgg.item()

# 2. Train with VGG Loss
def train_model(model, train_loader, val_loader, epochs=20, lr=1e-4):
    # Robust weights: L1 ensures color accuracy, VGG ensures sharpness
    criterion = VGGPerceptualLoss(l1_weight=1.0, vgg_weight=0.1, device=DEVICE)

    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    scaler = torch.amp.GradScaler('cuda')

    save_dir = Path("./checkpoints")
    save_dir.mkdir(exist_ok=True)

    for epoch in range(1, epochs + 1):
        model.train()
        epoch_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}")

        for noisy, clean in pbar:
            noisy, clean = noisy.to(DEVICE), clean.to(DEVICE)
            optimizer.zero_grad()

            with torch.amp.autocast('cuda'):
                pred = model(noisy)
                loss, l1_val, vgg_val = criterion(pred, clean)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            epoch_loss += loss.item()
            # Monitor L1 (Color) and VGG (Texture) separately
            pbar.set_postfix({"L1": f"{l1_val:.4f}", "VGG": f"{vgg_val:.4f}"})

        # Validation
        avg_loss = epoch_loss / len(train_loader)
        print(f"Epoch {epoch} Train Loss: {avg_loss:.5f}")

        if val_loader:
            model.eval()
            val_loss = 0
            with torch.no_grad():
                for v_noisy, v_clean in val_loader:
                    v_noisy, v_clean = v_noisy.to(DEVICE), v_clean.to(DEVICE)
                    v_pred = model(v_noisy)
                    v_total, _, _ = criterion(v_pred, v_clean)
                    val_loss += v_total.item()

            avg_val_loss = val_loss / len(val_loader)
            print(f"Epoch {epoch} Val Loss:   {avg_val_loss:.5f}")

            torch.save(model.state_dict(), save_dir / "last.pth")
            scheduler.step(avg_val_loss)

# 1. Create Datasets
# Enable training mode (cropping) for train, disable for validation
train_ds = PairedNISN(f"{BASE_PATH}/train/train", train_mode=True)
val_ds   = PairedNISN(f"{BASE_PATH}/validate/validate", train_mode=False)

# 2. Create Loaders
# TRAIN: We can now use a LARGER batch size (e.g. 64) because images are small (128px)
train_loader = DataLoader(
    train_ds,
    batch_size=64,      # Increased from 32 -> 64 for stable gradients
    shuffle=True,
    num_workers=2,
    pin_memory=True,
    persistent_workers=True
)

# VAL: We must use a SMALL batch size (e.g. 4) because images are huge (512px)
# If you run out of memory during validation, lower this to 1 or 2.
val_loader = DataLoader(
    val_ds,
    batch_size=4,       # Keep low to avoid GPU OOM on 512x512 images
    shuffle=False,
    num_workers=2,
    pin_memory=True,
    persistent_workers=True
)

# 3. Initialize & Train
model = UNet().to(DEVICE)
# Make sure you are using the 'Upsample' version of the UNet I gave you previously!
train_model(model, train_loader, val_loader, epochs=30, lr=2e-4)

import matplotlib.pyplot as plt

def visualize_results(model, loader, device='cuda', num_images=3):
    model.eval()

    # Get a single batch
    noisy_imgs, clean_imgs = next(iter(loader))
    noisy_imgs, clean_imgs = noisy_imgs.to(device), clean_imgs.to(device)

    # Generate predictions
    with torch.no_grad():
        preds = model(noisy_imgs)

    # Move to CPU for plotting
    noisy_imgs = noisy_imgs.cpu()
    clean_imgs = clean_imgs.cpu()
    preds = preds.cpu()

    plt.figure(figsize=(15, 5 * num_images))

    for i in range(num_images):
        # Noisy Input
        plt.subplot(num_images, 3, i*3 + 1)
        plt.imshow(noisy_imgs[i].permute(1, 2, 0).clamp(0, 1))
        plt.title("Input (Noisy)")
        plt.axis('off')

        # Prediction
        plt.subplot(num_images, 3, i*3 + 2)
        plt.imshow(preds[i].permute(1, 2, 0).clamp(0, 1))
        plt.title("Denoised Output")
        plt.axis('off')

        # Ground Truth
        plt.subplot(num_images, 3, i*3 + 3)
        plt.imshow(clean_imgs[i].permute(1, 2, 0).clamp(0, 1))
        plt.title("Ground Truth")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Run the visualization
visualize_results(model, val_loader, DEVICE)

train_model(model, train_loader, val_loader, epochs=30, lr=2e-4)

import torch
import shutil
from pathlib import Path

def save_model_to_drive(model, filename="denoising_unet_final.pth"):
    # 1. Define paths
    local_path = filename
    # You can change 'NSIN' to whatever folder you want in your Drive
    drive_folder = Path("/content/drive/MyDrive/NSIN/models")
    drive_path = drive_folder / filename

    # 2. Create the folder in Drive if it doesn't exist
    drive_folder.mkdir(parents=True, exist_ok=True)

    # 3. Save the model locally first (safer)
    print(f"Saving model locally to {local_path}...")
    torch.save(model.state_dict(), local_path)

    # 4. Copy to Google Drive
    print(f"Copying to Google Drive: {drive_path}...")
    shutil.copy(local_path, drive_path)

    print("‚úÖ Model saved successfully to Google Drive!")
    print(f"Path: {drive_path}")

# Run the save function
save_model_to_drive(model, "best_denoising_model.pth")

import os
import zipfile
import torch
from google.colab import files
from PIL import Image
import torchvision.transforms.functional as TF
import matplotlib.pyplot as plt
from pathlib import Path

def predict_from_zip(model, device='cuda'):
    print("üìÇ Please upload your 'predict_images.zip' file...")
    uploaded = files.upload()

    # 1. Find the zip file in the upload
    zip_name = None
    for name in uploaded.keys():
        if name.endswith('.zip'):
            zip_name = name
            break

    if not zip_name:
        print("‚ùå No zip file found. Please upload a .zip file.")
        return

    # 2. Unzip the folder
    print(f"extracting {zip_name}...")
    extract_path = Path("./predict_images_extracted")
    extract_path.mkdir(exist_ok=True)

    with zipfile.ZipFile(zip_name, 'r') as zip_ref:
        zip_ref.extractall(extract_path)

    # 3. Find all images (jpg, png, jpeg)
    image_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png']:
        image_files.extend(list(extract_path.rglob(ext)))

    print(f"‚úÖ Found {len(image_files)} images. Running inference...\n")

    # 4. Process each image
    model.eval()

    # Limit to first 10 to avoid crashing browser if folder is huge
    # Remove [:10] to process all
    for img_path in image_files[:10]:
        try:
            original = Image.open(img_path).convert("RGB")

            # Resize for U-Net compatibility (multiple of 16)
            w, h = original.size
            new_w = (w // 16) * 16
            new_h = (h // 16) * 16
            if new_w != w or new_h != h:
                original = original.resize((new_w, new_h))

            # Prepare Input
            input_tensor = TF.to_tensor(original).unsqueeze(0).to(device)

            # Run Inference
            with torch.no_grad():
                output_tensor = model(input_tensor)

            # Visualize
            input_disp = input_tensor.squeeze().cpu().permute(1, 2, 0).clamp(0, 1)
            output_disp = output_tensor.squeeze().cpu().permute(1, 2, 0).clamp(0, 1)

            plt.figure(figsize=(10, 5))
            plt.subplot(1, 2, 1)
            plt.imshow(input_disp)
            plt.title(f"Input: {img_path.name}")
            plt.axis('off')

            plt.subplot(1, 2, 2)
            plt.imshow(output_disp)
            plt.title("Denoised")
            plt.axis('off')

            plt.show()

        except Exception as e:
            print(f"‚ö†Ô∏è Could not process {img_path.name}: {e}")

# Run the tool
predict_from_zip(model, DEVICE)